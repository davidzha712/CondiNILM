# ============================================================
# NILMFormer: Full search space (multi-device) - includes 5min sampling rate
# ============================================================
NILMFormer:
  sampling_rate:
    type: categorical
    choices: ['6s','30s','1min','5min']
  window_size:
    type: categorical
    choices: [64, 128, 256]
  lr:
    type: loguniform
    low: 1e-5
    high: 5e-4
  wd:
    type: loguniform
    low: 1e-6
    high: 5e-2
  d_model:
    type: int
    low: 64
    high: 192
    step: 16
  n_encoder_layers:
    type: int
    low: 2
    high: 6
    step: 1
  n_head:
    type: int
    low: 2
    high: 8
    step: 2
  dp_rate:
    type: uniform
    low: 0.05
    high: 0.3
  pffn_ratio:
    type: int
    low: 2
    high: 6
    step: 1
  kernel_size:
    type: int
    low: 3
    high: 9
    step: 2
  kernel_size_head:
    type: int
    low: 1
    high: 5
    step: 2
  loss_lambda_energy:
    type: loguniform
    low: 0.05
    high: 5.0
  loss_alpha_on:
    type: loguniform
    low: 2.0
    high: 12.0
  loss_alpha_off:
    type: loguniform
    low: 0.05
    high: 1.2
  loss_lambda_on_recall:
    type: loguniform
    low: 1.0
    high: 8.0
  loss_on_recall_margin:
    type: uniform
    low: 0.5
    high: 0.95
  output_ratio:
    type: uniform
    low: 0.4
    high: 0.8
  gate_soft_scale:
    type: uniform
    low: 0.6
    high: 2.4
  gate_floor:
    type: loguniform
    low: 0.01
    high: 0.06
  loss_lambda_sparse:
    type: loguniform
    low: 0.0005
    high: 0.01
  loss_lambda_off_hard:
    type: loguniform
    low: 0.003
    high: 0.05

# ============================================================
# NILMFormer_Sparse: Optimized for sparse high-power devices (Microwave, Kettle)
# Features: higher alpha_on, lower alpha_off, higher gate_floor
# ============================================================
NILMFormer_Sparse:
  # Sampling rate and window - sparse devices may need finer sampling
  sampling_rate:
    type: categorical
    choices: ['6s','30s','1min']
  window_size:
    type: categorical
    choices: [64, 128]
  # Learning rate
  lr:
    type: loguniform
    low: 5e-5
    high: 3e-4
  wd:
    type: loguniform
    low: 1e-5
    high: 1e-2
  # Model architecture - sparse devices don't need large models
  d_model:
    type: int
    low: 64
    high: 128
    step: 32
  n_encoder_layers:
    type: int
    low: 2
    high: 4
    step: 1
  n_head:
    type: int
    low: 2
    high: 4
    step: 2
  dp_rate:
    type: uniform
    low: 0.1
    high: 0.3
  pffn_ratio:
    type: int
    low: 2
    high: 4
    step: 1
  kernel_size:
    type: int
    low: 3
    high: 7
    step: 2
  kernel_size_head:
    type: int
    low: 1
    high: 3
    step: 2
  # ===== Key loss params - optimized for sparse devices =====
  # ON-state weight - significantly increased to detect sparse events
  loss_alpha_on:
    type: loguniform
    low: 6.0
    high: 18.0
  # OFF-state weight - significantly reduced to avoid over-penalization
  loss_alpha_off:
    type: loguniform
    low: 0.05
    high: 0.5
  # Recall weight - increased to reduce missed detections
  loss_lambda_on_recall:
    type: loguniform
    low: 2.0
    high: 8.0
  loss_on_recall_margin:
    type: uniform
    low: 0.85
    high: 0.98
  # Energy regression
  loss_lambda_energy:
    type: loguniform
    low: 0.05
    high: 0.5
  # Sparsity penalty - reduced to allow more activations
  loss_lambda_sparse:
    type: loguniform
    low: 0.0001
    high: 0.003
  # OFF hard constraint - reduced
  loss_lambda_off_hard:
    type: loguniform
    low: 0.001
    high: 0.02
  # ===== Gate params - critical! =====
  # Gate sharpness
  gate_soft_scale:
    type: uniform
    low: 1.5
    high: 4.0
  # Gate floor - increased to allow more detections
  gate_floor:
    type: loguniform
    low: 0.015
    high: 0.1
  # Output ratio
  output_ratio:
    type: uniform
    low: 0.5
    high: 0.75

# ============================================================
# NILMFormer_Precision: Optimized for improving Precision (reduce false positives)
# Target: Microwave (current Precision=0.075, needs significant improvement)
# Strategy: enhance OFF penalty, tighten gate control, reduce over-detection
# ============================================================
NILMFormer_Precision:
  sampling_rate:
    type: categorical
    choices: ["1min"]
  window_size:
    type: categorical
    choices: [128]
  lr:
    type: loguniform
    low: 3e-5
    high: 2e-4
  wd:
    type: loguniform
    low: 1e-5
    high: 5e-3
  d_model:
    type: int
    low: 64
    high: 128
    step: 32
  n_encoder_layers:
    type: int
    low: 2
    high: 4
    step: 1
  n_head:
    type: int
    low: 2
    high: 4
    step: 2
  dp_rate:
    type: uniform
    low: 0.1
    high: 0.25
  pffn_ratio:
    type: int
    low: 2
    high: 4
    step: 1
  kernel_size:
    type: int
    low: 3
    high: 7
    step: 2
  kernel_size_head:
    type: int
    low: 1
    high: 3
    step: 2
  # ===== Key: params for improving Precision =====
  # ON weight - moderate, don't go too high
  loss_alpha_on:
    type: loguniform
    low: 4.0
    high: 10.0
  # OFF weight - significantly increased to penalize false positives!
  loss_alpha_off:
    type: loguniform
    low: 0.3
    high: 2.0
  # Recall weight - moderate
  loss_lambda_on_recall:
    type: loguniform
    low: 1.0
    high: 4.0
  loss_on_recall_margin:
    type: uniform
    low: 0.7
    high: 0.95
  # OFF hard constraint - significantly increased!
  loss_lambda_off_hard:
    type: loguniform
    low: 0.02
    high: 0.2
  # Sparsity penalty - increased to reduce over-activation
  loss_lambda_sparse:
    type: loguniform
    low: 0.001
    high: 0.02
  # Energy regression
  loss_lambda_energy:
    type: loguniform
    low: 0.1
    high: 0.5
  # Gate sharpness - increased for clearer ON/OFF separation
  gate_soft_scale:
    type: uniform
    low: 2.0
    high: 5.0
  # Gate floor - reduced to minimize OFF-state leakage!
  gate_floor:
    type: loguniform
    low: 0.005
    high: 0.04
  output_ratio:
    type: uniform
    low: 0.5
    high: 0.7
  # Zero penalty - increased to penalize non-zero output during OFF state
  state_zero_penalty_weight:
    type: loguniform
    low: 0.05
    high: 0.3
  # OFF penalty weight
  off_high_agg_penalty_weight:
    type: loguniform
    low: 0.1
    high: 0.5

# ============================================================
# NILMFormer_REFIT: Optimized for REFIT dataset
# REFIT native sampling rate 8s (not UKDALE's 6s), 4 devices: DW, WM, Kettle, Fridge
# Note: ds_loss params like output_ratio/anti_collapse are protected by smart HPO guard
# ============================================================
NILMFormer_REFIT:
  # REFIT native 8s -> options: 8s/30s/1min (no 6s/5min)
  sampling_rate:
    type: categorical
    choices: ['8s', '30s', '1min']
  # Window size: must cover sufficient device cycle lengths
  # 8s*256=34min, 30s*128=64min, 1min*128=128min
  window_size:
    type: categorical
    choices: [64, 128, 256]
  # Learning rate
  lr:
    type: loguniform
    low: 5e-5
    high: 5e-4
  wd:
    type: loguniform
    low: 1e-6
    high: 5e-2
  # Model architecture (same range as general NILMFormer)
  d_model:
    type: int
    low: 64
    high: 192
    step: 16
  n_encoder_layers:
    type: int
    low: 2
    high: 6
    step: 1
  n_head:
    type: int
    low: 2
    high: 8
    step: 2
  dp_rate:
    type: uniform
    low: 0.05
    high: 0.3
  pffn_ratio:
    type: int
    low: 2
    high: 6
    step: 1
  kernel_size:
    type: int
    low: 3
    high: 9
    step: 2
  kernel_size_head:
    type: int
    low: 1
    high: 5
    step: 2
  # ===== Loss params (only search params NOT protected by ds_loss) =====
  # Smart HPO guard skips REFIT ds_loss keys:
  #   output_ratio, anti_collapse_weight, state_zero_penalty_weight,
  #   state_zero_kernel, off_high_agg_penalty_weight
  # The following params are passed to training:
  loss_lambda_energy:
    type: loguniform
    low: 0.05
    high: 5.0
  loss_alpha_on:
    type: loguniform
    low: 2.0
    high: 12.0
  loss_alpha_off:
    type: loguniform
    low: 0.05
    high: 1.2
  loss_lambda_on_recall:
    type: loguniform
    low: 1.0
    high: 8.0
  loss_on_recall_margin:
    type: uniform
    low: 0.5
    high: 0.95
  gate_soft_scale:
    type: uniform
    low: 0.6
    high: 2.4
  gate_floor:
    type: loguniform
    low: 0.01
    high: 0.06
  loss_lambda_sparse:
    type: loguniform
    low: 0.0005
    high: 0.01
  loss_lambda_off_hard:
    type: loguniform
    low: 0.003
    high: 0.05
