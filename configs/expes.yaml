data_path:  data/
result_path:  result/
dataset:  UKDALE
name_model:  NILMFormer
sampling_rate:  1min
window_size: 128
list_exo_variables:
  - minute
  - hour
  - dow
  - month

power_scaling_type:  MaxScaling
appliance_scaling_type:  MaxScaling

batch_size:  2048                # V11: Fill 5090 32GB VRAM (~17GB peak, full-data still has ~63 steps/epoch)
epochs:  50                      # V10: More epochs for full-data training (was 25 with 10% data)
p_es:  12                        # V10: Slightly more patience for full-data training
p_rlr:  5                        # V10: More patience before LR reduction
n_warmup_epochs:  3              # V35c: Match T5 (was 5)
scheduler_type:  cosine_warmup   # cosine_warmup (recommended) / plateau
rlr_min_lr:  1e-6

device:  auto          # auto / cuda / mps / cpu
accumulate_grad_batches: 1       # V12: PCGrad uses manual optimization
limit_train_batches: 1.0         # V10: USE ALL DATA — previous 0.1 starved the model
limit_val_batches: 1.0           # V10: Full validation for reliable early stopping

num_workers: 8
prefetch_factor: 4
check_val_every_n_epoch: 2       # V12: Reduce validation overhead (callbacks do 2x forward + full postprocess per val epoch)

train_num_crops: 4               # V11: Keep 4 crops to maintain ~98% VRAM utilization (batch=2048 * 4 = 8192 effective)
train_crop_ratio: 0.75           # V11: More aggressive cropping → more diversity per sample
train_crop_event_bias: 0.8       # V35c: Match T5. Higher event bias → more WM ON events per batch

# V8: Disable aggressive sparse oversampling (causes multi-task negative transfer)
balance_window_sampling: false

# EAEC parameters removed (V12+: replaced by AdaptiveDeviceLoss / multi_nilm)

# seq2subseq: Only supervise center region of output to reduce boundary effects
# seq2subseq: Only supervise center region of output to reduce boundary effects
output_ratio: 0.75              # V35c: Match T5

# V32b: Global penalties still disabled — V32 showed they hurt FR badly with batch=2048.
# The real T5 difference is batch_size (128 vs 2048), not penalty weights.
state_zero_penalty_weight: 0.0
state_zero_kernel: 0
state_zero_ratio: 0.0

postprocess_min_on_steps: 3

off_high_agg_penalty_weight: 0.0

anti_collapse_weight: 0.5        # V35c: Match T5

# ============== Loss function hyperparameters (auto-adjusted by appliance type) ==============
# These are default values and will be overridden by _configure_nilm_loss_hyperparams based on appliance type
#
# Supported appliance types:
#   - sparse_high_power:    sparse high-power appliances (e.g., Kettle, Microwave)
#   - frequent_switching:   frequently switching appliances (e.g., Fridge)
#   - long_cycle:           long-cycle appliances (e.g., WashingMachine, Dishwasher)
#   - always_on:            always-on appliances
#   - sparse_medium_power:  sparse medium-power appliances
#
# Uses AdaptiveDeviceLoss (multi_nilm) - auto-adjusts params by device type
loss_type: multi_nilm

# ===== HPO best params (Trial 4, weighted F1=0.527) =====
# V10: These are multiplicative scales applied to AdaptiveDeviceLoss base params.
# Previous values were tuned for 10% data, causing extreme amplification.
# With full data, use conservative scales to let AdaptiveDeviceLoss's internal params work.

# lambda_off_hard: OFF-state hard constraint weight
loss_lambda_off_hard: 0.01368   # V35e: Match T5

# lambda_sparse: sparsity penalty
loss_lambda_sparse: 0.00725     # V35e: Match T5

# off_margin: power tolerance margin in OFF state
loss_off_margin: 0.02

# ===== ON missed-detection penalty (critical!) =====
# V35e: Restored to T5 values. V10's reduction (5.368→2.0, 2.67→1.5) was wrong:
# These are CAPPED in training.py (recall→min(val,3.0), alpha→min(val/2,1.5))
# T5 effective: recall_scale=3.0, alpha_on_scale=1.335 → WM stable.
# V35d effective: recall_scale=2.0, alpha_on_scale=0.75 → WM collapsed.
loss_lambda_on_recall: 5.368

loss_alpha_on: 2.67
loss_alpha_off: 0.96

# on_recall_margin: minimum fraction of target output when ON
loss_on_recall_margin: 0.785

# lambda_energy: energy regression weight
loss_lambda_energy: 0.19

# ===== Regression Enhancement Parameters (V3) =====
# These parameters improve power regression while maintaining classification
#
# w_peak: Peak-aware loss weight (captures compressor start-up peaks for Fridge)
#   - Higher values (0.15-0.20) for cycling devices that have distinct peaks
#   - Lower values (0.05-0.10) for sparse devices
loss_w_peak: 0.12

# w_grad: Gradient smoothness loss weight (temporal consistency)
#   - Higher values (0.10-0.15) for devices with smooth power curves
#   - Lower values (0.05-0.08) for devices with rapid power changes
loss_w_grad: 0.08

# w_range: Power range constraint weight (prevents over-prediction)
#   - Higher values (0.12-0.15) for sparse devices prone to false positives
#   - Lower values (0.05-0.08) for devices with stable power
loss_w_range: 0.10

# w_on_power: ON power accuracy weight (relative error during ON)
#   - Higher values improve NDE metric
loss_w_on_power: 0.12

# ===== Gate classification (TWO-STAGE approach for sparse devices) =====
# V35e: Restored to T5 value (was 0.5)
gate_cls_weight: 0.3

# lambda_gate_cls: legacy parameter (keep for compatibility)
loss_lambda_gate_cls: 0.5

# gate_focal_gamma: Focal Loss gamma parameter
loss_gate_focal_gamma: 2.0

# ===== Soft Gating Parameters (HPO optimized) =====
# gate_soft_scale: Sigmoid sharpness
gate_soft_scale: 2.05

# gate_floor: Minimum gate probability
gate_floor: 0.014

postprocess_gate_avg_threshold: 0.25
postprocess_gate_max_threshold: 0.5

# NOTE: Per-device postprocess params are in dataset_params.yaml (overrides this file).

# ============== Gradient Conflict Resolution V2 (Multi-Device Training) ==============
# PCGrad + Gradient Balancing to solve gradient conflicts in multi-device training.
#
# Problem: In 4-device training, some devices collapse (F1=0) due to:
# 1. 26x gradient magnitude differences (alpha_on=8.0 vs alpha_off=3.5)
# 2. Gradient direction conflicts (sparse devices want recall UP, others want DOWN)
# 3. Shared encoder receives conflicting gradients that cancel out
#
# Solution V2 (Improved):
# - Soft gradient balancing: Reduce extreme ratios while preserving relative importance
# - PCGrad with randomized order: Avoid systematic bias in projection
# - Magnitude restoration: Maintain learning signal strength after projection
#
# Reference: Yu et al. "Gradient Surgery for Multi-Task Learning" (NeurIPS 2020)
#
# Enable this for multi-device training (>1 appliance):
# NOTE: Disabled when using gradient_isolation (more aggressive isolation)
use_gradient_conflict_resolution: true   # V12: Re-enabled (essential for training stability — disabling causes mode collapse by epoch 3)
pcgrad_every_n_steps: 1                  # V28: Every step (was 2). Non-PCGrad steps allow WM all-ON collapse — T5 used 1 and didn't collapse

# PCGrad sub-options:
# - use_pcgrad: Apply gradient projection when conflicts detected (default: true)
# - use_normalization: Balance gradients before aggregation (default: true)
# - conflict_threshold: Cosine threshold for conflict detection (default: 0.0)
gradient_conflict_use_pcgrad: true
gradient_conflict_use_normalization: true
gradient_conflict_threshold: 0.0

# V2 New options:
# - balance_method: How to balance gradient magnitudes
#   - "soft": Reduce extreme ratios while preserving importance (RECOMMENDED)
#   - "unit": Normalize to unit length (original behavior, may cause instability)
#   - "none": No balancing
# - balance_max_ratio: Maximum allowed ratio between largest/smallest gradients (for soft method)
# - randomize_order: Randomize device order in PCGrad to avoid systematic bias
gradient_conflict_balance_method: soft
gradient_conflict_balance_max_ratio: 2.0  # REDUCED from 3.0 - tighter balance for sparse devices
gradient_conflict_randomize_order: true

# ============== Gradient Isolation V1 (Complete Device Separation) ==============
# For multi-device training where PCGrad is not enough.
#
# Problem: Even with PCGrad, some devices (microwave) collapse because:
# 1. Kettle dominates the shared backbone optimization
# 2. Microwave's gradients are overwritten by kettle's stronger signal
#
# Solution: Completely isolate device heads
# - Each device's loss ONLY updates its own adapter and head parameters
# - Shared backbone can be frozen or receive averaged gradients
#
# This is more aggressive than PCGrad: devices train TRULY independently.
#
# Enable this for multi-device training with collapse issues:
use_gradient_isolation: false   # V35c: Match T5 (false). Isolation prevents WM from getting shared backbone features.

# Backbone training mode:
# - "frozen": Shared backbone receives NO gradients (most isolated, devices are independent)
# - "average": Shared backbone receives AVERAGED gradients from all devices
# - "anchor": Only non-isolated devices update the backbone
gradient_isolation_backbone: average

# Devices to isolate (comma-separated). Empty = ALL devices isolated.
# Example: "microwave,kettle" to only isolate sparse devices
# When using "anchor" mode, devices NOT in this list will update the backbone.
gradient_isolation_devices: ""
