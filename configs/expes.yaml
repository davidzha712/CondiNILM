data_path:  data/
result_path:  result/
dataset:  UKDALE
name_model:  NILMFormer
sampling_rate:  1min
window_size: 128
list_exo_variables:
  - minute
  - hour
  - dow
  - month

power_scaling_type:  MaxScaling
appliance_scaling_type:  MaxScaling

batch_size:  128
epochs:  25
p_es:  5                         # Early stopping patience (epochs without improvement)
p_rlr:  3
n_warmup_epochs:  3
scheduler_type:  cosine_warmup   # cosine_warmup (recommended) / plateau
rlr_min_lr:  1e-6

device:  auto          # auto / cuda / mps / cpu
accumulate_grad_batches: 1
limit_train_batches: 0.1
limit_val_batches: 0.2

num_workers: 8
prefetch_factor: 4

train_num_crops: 4
train_crop_ratio: 0.75
train_crop_event_bias: 0.8

# EAEC Related Parameters
p_es_eaec:  null
p_rlr_eaec:  5
n_warmup_epochs_eaec:  3
warmup_type_eaec:  linear
neg_penalty_weight_eaec: 0.02
rlr_factor_eaec: 0.5
rlr_min_lr_eaec: 1e-6
gradient_clip_val_eaec: 1.0
gate_cls_weight_eaec: 1.0
gate_window_weight_eaec: 0.5
gate_focal_gamma_eaec: 2.0

# seq2subseq: Only supervise center region of output to reduce boundary effects
# HPO best: 0.75 (supervise middle 75%)
output_ratio: 0.75

# Zero-run penalty for target=0 segments
state_zero_penalty_weight: 0.1
state_zero_kernel: 48
state_zero_ratio: 0.9

postprocess_min_on_steps: 3

off_high_agg_penalty_weight: 0.3

# Anti-collapse penalty to prevent model from outputting all zeros
# Reduced from 1.5 to 0.8 - too high causes over-prediction
anti_collapse_weight: 0.8

# ============== Loss function hyperparameters (auto-adjusted by appliance type) ==============
# These are default values and will be overridden by _configure_nilm_loss_hyperparams based on appliance type
#
# Supported appliance types:
#   - sparse_high_power:    sparse high-power appliances (e.g., Kettle, Microwave)
#   - frequent_switching:   frequently switching appliances (e.g., Fridge)
#   - long_cycle:           long-cycle appliances (e.g., WashingMachine, Dishwasher)
#   - always_on:            always-on appliances
#   - sparse_medium_power:  sparse medium-power appliances
#
# 使用 AdaptiveDeviceLoss (multi_nilm) - 自动根据设备类型调整参数
loss_type: multi_nilm

# ===== HPO最佳参数 (Trial 4, weighted F1=0.527) =====
# lambda_off_hard: OFF-state hard constraint weight
loss_lambda_off_hard: 0.01368

# lambda_sparse: sparsity penalty
loss_lambda_sparse: 0.00725

# off_margin: power tolerance margin in OFF state
loss_off_margin: 0.02

# ===== ON missed-detection penalty (critical!) =====
# lambda_on_recall: ON missed-detection penalty weight
loss_lambda_on_recall: 5.368

# alpha_on/alpha_off: class weighting for ON/OFF
loss_alpha_on: 2.67
loss_alpha_off: 0.96

# on_recall_margin: minimum fraction of target output when ON
loss_on_recall_margin: 0.785

# lambda_energy: energy regression weight
loss_lambda_energy: 0.19

# ===== Regression Enhancement Parameters (V3) =====
# These parameters improve power regression while maintaining classification
#
# w_peak: Peak-aware loss weight (captures compressor start-up peaks for Fridge)
#   - Higher values (0.15-0.20) for cycling devices that have distinct peaks
#   - Lower values (0.05-0.10) for sparse devices
loss_w_peak: 0.12

# w_grad: Gradient smoothness loss weight (temporal consistency)
#   - Higher values (0.10-0.15) for devices with smooth power curves
#   - Lower values (0.05-0.08) for devices with rapid power changes
loss_w_grad: 0.08

# w_range: Power range constraint weight (prevents over-prediction)
#   - Higher values (0.12-0.15) for sparse devices prone to false positives
#   - Lower values (0.05-0.08) for devices with stable power
loss_w_range: 0.10

# w_on_power: ON power accuracy weight (relative error during ON)
#   - Higher values improve NDE metric
loss_w_on_power: 0.12

# ===== Gate classification (TWO-STAGE approach for sparse devices) =====
# gate_cls_weight: EXPLICIT gate classification loss weight (overrides loss_lambda_gate_cls)
# Set high (1.0) to prioritize binary ON/OFF classification before power regression
gate_cls_weight: 1.0

# lambda_gate_cls: legacy parameter (keep for compatibility)
loss_lambda_gate_cls: 0.5

# gate_focal_gamma: Focal Loss gamma parameter
loss_gate_focal_gamma: 2.0

# ===== Soft Gating Parameters (HPO optimized) =====
# gate_soft_scale: Sigmoid sharpness
gate_soft_scale: 2.05

# gate_floor: Minimum gate probability
gate_floor: 0.014

postprocess_gate_avg_threshold: 0.25
postprocess_gate_max_threshold: 0.5

# OPTIMIZED: 后处理参数 - V5: 更严格的OFF抑制
postprocess_per_device:
  # Microwave: 适度阈值抑制OFF噪声
  microwave:
    postprocess_threshold: 35     # V5: Raised from 30 to reduce OFF noise
    postprocess_min_on_steps: 1   # 保持敏感性
  # Kettle: 提高阈值
  Kettle:
    postprocess_threshold: 25     # V5: Raised from 20
    postprocess_min_on_steps: 2
  # Fridge: 提高阈值压制OFF噪声
  Fridge:
    postprocess_threshold: 50     # V5: Raised from 40
    postprocess_min_on_steps: 3
  # WashingMachine: 更激进压制假阳性
  WashingMachine:
    postprocess_threshold: 75     # V5: Raised from 60
    postprocess_min_on_steps: 10  # V5: Reduced for better response
  # Dishwasher: 更激进压制假阳性
  Dishwasher:
    postprocess_threshold: 55     # V5: Raised from 45
    postprocess_min_on_steps: 10  # V5: Reduced

# ============== Gradient Conflict Resolution V2 (Multi-Device Training) ==============
# PCGrad + Gradient Balancing to solve gradient conflicts in multi-device training.
#
# Problem: In 4-device training, some devices collapse (F1=0) due to:
# 1. 26x gradient magnitude differences (alpha_on=8.0 vs alpha_off=3.5)
# 2. Gradient direction conflicts (sparse devices want recall UP, others want DOWN)
# 3. Shared encoder receives conflicting gradients that cancel out
#
# Solution V2 (Improved):
# - Soft gradient balancing: Reduce extreme ratios while preserving relative importance
# - PCGrad with randomized order: Avoid systematic bias in projection
# - Magnitude restoration: Maintain learning signal strength after projection
#
# Reference: Yu et al. "Gradient Surgery for Multi-Task Learning" (NeurIPS 2020)
#
# Enable this for multi-device training (>1 appliance):
# NOTE: Disabled when using gradient_isolation (more aggressive isolation)
use_gradient_conflict_resolution: false

# PCGrad sub-options:
# - use_pcgrad: Apply gradient projection when conflicts detected (default: true)
# - use_normalization: Balance gradients before aggregation (default: true)
# - conflict_threshold: Cosine threshold for conflict detection (default: 0.0)
gradient_conflict_use_pcgrad: true
gradient_conflict_use_normalization: true
gradient_conflict_threshold: 0.0

# V2 New options:
# - balance_method: How to balance gradient magnitudes
#   - "soft": Reduce extreme ratios while preserving importance (RECOMMENDED)
#   - "unit": Normalize to unit length (original behavior, may cause instability)
#   - "none": No balancing
# - balance_max_ratio: Maximum allowed ratio between largest/smallest gradients (for soft method)
# - randomize_order: Randomize device order in PCGrad to avoid systematic bias
gradient_conflict_balance_method: soft
gradient_conflict_balance_max_ratio: 2.0  # REDUCED from 3.0 - tighter balance for sparse devices
gradient_conflict_randomize_order: true

# ============== Gradient Isolation V1 (Complete Device Separation) ==============
# For multi-device training where PCGrad is not enough.
#
# Problem: Even with PCGrad, some devices (microwave) collapse because:
# 1. Kettle dominates the shared backbone optimization
# 2. Microwave's gradients are overwritten by kettle's stronger signal
#
# Solution: Completely isolate device heads
# - Each device's loss ONLY updates its own adapter and head parameters
# - Shared backbone can be frozen or receive averaged gradients
#
# This is more aggressive than PCGrad: devices train TRULY independently.
#
# Enable this for multi-device training with collapse issues:
use_gradient_isolation: true

# Backbone training mode:
# - "frozen": Shared backbone receives NO gradients (most isolated, devices are independent)
# - "average": Shared backbone receives AVERAGED gradients from all devices
# - "anchor": Only non-isolated devices update the backbone
gradient_isolation_backbone: average

# Devices to isolate (comma-separated). Empty = ALL devices isolated.
# Example: "microwave,kettle" to only isolate sparse devices
# When using "anchor" mode, devices NOT in this list will update the backbone.
gradient_isolation_devices: ""
