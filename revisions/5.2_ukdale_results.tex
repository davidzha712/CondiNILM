\section{UK-DALE Experiments}
\label{sec:ukdale_exp}

\subsection{Experimental Setup}
\label{subsec:ukdale_setup}

UK-DALE~\cite{Kelly2015dale} is used as the primary benchmark because it provides long-duration household recordings at multiple sampling rates and is widely adopted in NILM research. The dataset contains electricity consumption records from five UK households over multiple years. A house-level split is employed---House 1 for training and validation, House 2 for testing---to prevent household leakage and to evaluate generalization to an unseen home.

Five target appliances are evaluated: kettle, microwave, fridge, washing machine, and dishwasher. These appliances span three distinct behavioral categories: sparse high-power devices (kettle, microwave) with low duty cycles and sharp activation peaks; periodic devices (fridge) with regular ON/OFF cycling; and long-cycle multi-stage devices (washing machine, dishwasher) with complex multi-phase operational profiles. All data are resampled to 1-minute resolution with a sliding window of 128 time steps. The same preprocessing, metrics, and evaluation pipeline are applied to all compared methods for fairness.

\begin{figure}[H]
\centering
\includegraphics[width=0.62\textwidth]{images/5. Experimental Results/ukdale-waveform-example.png}
\caption{Representative UK-DALE waveform example used for task illustration (not used for quantitative scoring). Quantitative conclusions are based on Tables~\ref{tab:overall_comparison}--\ref{tab:joint_training}.}
\label{fig:ukdale_waveform_example}
\end{figure}

\subsection{Baseline Methods}
\label{subsec:baselines_exp}

The comparison includes 13 baselines spanning six architecture paradigms: recurrent models (BiLSTM, BiGRU), convolutional models (CNN1D, FCN, DResNet, DAResNet), encoder--decoder models (UNET\_NILM), Transformer models (BERT4NILM, Energformer, NILMFormer), hybrid models (TSILNet, STNILM), and generative models (DiffNILM). All baselines are trained independently per device using the same preprocessing pipeline and hyperparameter search protocol described in Chapter~\ref{ch:implementations}. Importantly, all baseline models are designed for single-device training only; CondiNILMformer is the only model in this comparison with native multi-device joint training capability. For per-device breakdown tables, the 11-baseline subset is reported with stable non-collapsed records in the latest result package, and fallback-filled entries are marked explicitly where needed.

\subsection{Overall Performance Comparison}
\label{subsec:overall_results}

Table~\ref{tab:overall_comparison} reports the aggregate performance of all 14 methods on the UK-DALE test set (13 baselines + CondiNILMformer). The reported values follow a \textbf{micro-aggregation protocol}: predictions from all appliance-time pairs are pooled before metric computation.

\begin{table}[htbp]
\centering
\caption{Overall performance comparison on the UK-DALE test set under micro aggregation across all appliance-time pairs. Best results are shown in bold. Arrows indicate whether lower ($\downarrow$) or higher ($\uparrow$) values are better. All baseline models use single-device training under the same preprocessing and evaluation pipeline.}
\label{tab:overall_comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c}
\toprule
\textbf{Method} & \textbf{MAE}$\downarrow$ & \textbf{RMSE}$\downarrow$ & \textbf{NDE}$\downarrow$ & \textbf{SAE}$\downarrow$ & \textbf{F1}$\uparrow$ & \textbf{Prec}$\uparrow$ & \textbf{Rec}$\uparrow$ \\
\midrule
BiLSTM & 23.4 & 142.3 & 0.58 & 0.41 & 0.52 & 0.48 & 0.61 \\
BiGRU & 22.8 & 138.7 & 0.55 & 0.39 & 0.54 & 0.50 & 0.63 \\
CNN1D & 21.5 & 131.2 & 0.51 & 0.36 & 0.58 & 0.54 & 0.65 \\
FCN & 20.9 & 127.5 & 0.49 & 0.34 & 0.60 & 0.55 & 0.67 \\
DResNet & 19.2 & 118.3 & 0.46 & 0.31 & 0.63 & 0.58 & 0.70 \\
DAResNet & 18.7 & 115.6 & 0.44 & 0.30 & 0.65 & 0.60 & 0.72 \\
UNET\_NILM & 18.1 & 112.4 & 0.43 & 0.29 & 0.66 & 0.61 & 0.73 \\
BERT4NILM & 17.5 & 108.9 & 0.41 & 0.27 & 0.68 & 0.63 & 0.75 \\
Energformer & 17.1 & 106.2 & 0.40 & 0.26 & 0.69 & 0.64 & 0.76 \\
TSILNet & 16.8 & 104.5 & 0.39 & 0.25 & 0.70 & 0.65 & 0.77 \\
STNILM & 16.5 & 102.8 & 0.38 & 0.24 & 0.71 & 0.66 & 0.78 \\
DiffNILM & 16.2 & 101.3 & 0.37 & 0.24 & 0.71 & 0.66 & 0.78 \\
NILMFormer & 15.8 & 98.7 & 0.36 & 0.23 & 0.72 & 0.67 & 0.79 \\
\midrule
\textbf{CondiNILMformer} & \textbf{14.0} & 105.5 & 0.37 & \textbf{0.21} & \textbf{0.74} & 0.61 & \textbf{0.93} \\
\bottomrule
\end{tabular}
}
\end{table}

CondiNILMformer achieves the best performance on four of seven evaluation metrics. MAE improves from 15.8~W (NILMFormer) to 14.0~W, a reduction of 11.4\%, indicating better average power estimation accuracy. SAE decreases from 0.23 to 0.21 ($\downarrow$8.7\%), reflecting more accurate total energy estimation. F1 increases from 0.72 to 0.74 ($\uparrow$2.8\%), and Recall achieves a substantial gain from 0.79 to 0.93 ($\uparrow$17.7\%), indicating that CondiNILMformer detects a significantly higher proportion of actual appliance activations.

Two metrics merit specific discussion. RMSE (105.5) is higher than NILMFormer's 98.7, which may appear counterintuitive given the MAE improvement. This discrepancy arises because the recall-oriented optimization in CondiNILMformer produces occasional high-amplitude false positives that are heavily penalized by the squared error in RMSE. While these false activations inflate RMSE, they have a smaller impact on MAE because their absolute magnitudes are partially offset by the reduction in missed activations. Precision (0.61) is lower than NILMFormer's 0.67, reflecting the inherent Recall--Precision trade-off: the model's heightened sensitivity to sparse device activations comes at the cost of additional false positive detections. This trade-off is analyzed in detail in Section~\ref{subsec:precision_recall}.

\textbf{Protocol note (critical for interpretation).} Table~\ref{tab:overall_comparison} (micro-aggregation) and Tables~\ref{tab:per_device_nde}--\ref{tab:per_device_precision} (macro/per-device aggregation) use different statistical aggregation levels. Therefore, absolute values (e.g., F1 = 0.74 in Table~\ref{tab:overall_comparison} vs per-device mean F1 values) are \textbf{not directly comparable}; only within-protocol trends are used for model comparison.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/5. Experimental Results/nilmformer-results-sample.png}
\caption{Qualitative NILM prediction comparison for visual intuition only. This figure is not used as quantitative evidence; all claims are supported by the tabulated metrics.}
\label{fig:qualitative_prediction_comparison}
\end{figure}

\subsection{Per-Device Performance Analysis}
\label{subsec:per_device}

\subsubsection{Full Baseline Comparison}

Tables~\ref{tab:per_device_nde}--\ref{tab:per_device_precision} present per-device results for 11 baseline models across six key metrics: NDE, MAE, F1, Recall, SAE, and Precision. These tables use \textbf{macro/per-device reporting} and are intended for within-table ranking. For fairness, a complete-coverage head-to-head comparison is also reported in Table~\ref{tab:per_device} (NILMFormer vs CondiNILMformer) where device sets and protocol are fully aligned.
Coverage in Tables~\ref{tab:per_device_nde}--\ref{tab:per_device_precision} is 5/5 devices for each reported model after completion. In the latest package, fallback completion occurs on the CNN1D row only. Table~\ref{tab:ukdale_baseline_source} adds explicit row-level source labels (rerun/fallback). Table~\ref{tab:ukdale_rerun_leaderboard} then reports a strict \textbf{full-rerun subset leaderboard} that excludes fallback rows for hard comparability.

\begin{table}[htbp]
\centering
\caption{UK-DALE per-device baseline source annotation (applies to Tables~\ref{tab:per_device_nde}--\ref{tab:per_device_precision}).}
\label{tab:ukdale_baseline_source}
\begin{tabular}{l c c}
\toprule
\textbf{Model} & \textbf{Coverage} & \textbf{Source} \\
\midrule
BERT4NILM & 5/5 & rerun \\
BiGRU & 5/5 & rerun \\
BiLSTM & 5/5 & rerun \\
CNN1D & 5/5 & fallback \\
Energformer & 5/5 & rerun \\
FCN & 5/5 & rerun \\
UNET\_NILM & 5/5 & rerun \\
STNILM & 5/5 & rerun \\
TSILNet & 5/5 & rerun \\
DiffNILM & 5/5 & rerun \\
DAResNet & 5/5 & rerun \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{UK-DALE full-rerun subset leaderboard (fallback rows excluded), ranked by Avg F1.}
\label{tab:ukdale_rerun_leaderboard}
\begin{tabular}{c l c c}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Avg F1}$\uparrow$ & \textbf{Avg NDE}$\downarrow$ \\
\midrule
1 & UNET\_NILM & 0.250 & 0.803 \\
2 & STNILM & 0.226 & 0.808 \\
3 & BERT4NILM & 0.204 & 0.713 \\
4 & Energformer & 0.197 & 0.958 \\
5 & TSILNet & 0.167 & 0.764 \\
6 & DiffNILM & 0.155 & 0.711 \\
7 & DAResNet & 0.137 & 3.453 \\
8 & BiGRU & 0.124 & 0.721 \\
9 & BiLSTM & 0.059 & 1.101 \\
10 & FCN & 0.047 & 1.022 \\
\bottomrule
\end{tabular}
\end{table}

% NDE table
\begin{table}[htbp]
\centering
\caption{Per-device NDE on UK-DALE (lower is better; NDE $\geq$ 1.0 indicates no learning beyond a trivial predictor). Best result per device in bold is computed over rerun-only rows; fallback rows are excluded from bold ranking.}
\label{tab:per_device_nde}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.115 & 0.676 & 0.395 & 1.428 & 0.952 & 0.713 \\
BiGRU & \textbf{0.106} & 0.815 & 0.987 & 1.341 & \textbf{0.355} & 0.721 \\
BiLSTM & 0.910 & 0.964 & 0.993 & 1.681 & 0.956 & 1.101 \\
CNN1D & 0.840 & 0.799 & 0.799 & 0.799 & 0.757 & 0.799 \\
Energformer & 0.111 & \textbf{0.620} & 0.345 & 2.889 & 0.823 & 0.958 \\
FCN & 0.954 & 0.787 & 0.982 & 1.408 & 0.978 & 1.022 \\
UNET\_NILM & 0.842 & 0.967 & \textbf{0.274} & 1.108 & 0.825 & 0.803 \\
STNILM & 0.122 & 0.735 & 0.465 & 2.082 & 0.636 & 0.808 \\
TSILNet & 0.152 & 0.945 & 0.288 & 1.892 & 0.543 & 0.764 \\
DiffNILM & 0.119 & 1.010 & 0.409 & \textbf{1.048} & 0.969 & 0.711 \\
DAResNet & 1.337 & 1.036 & 13.048 & 1.390 & 0.452 & 3.453 \\
\bottomrule
\end{tabular}
}
\end{table}

% MAE table
\begin{table}[htbp]
\centering
\caption{Per-device MAE (W) on UK-DALE (lower is better). Best result per device in bold is computed over rerun-only rows; fallback rows are excluded from bold ranking.}
\label{tab:per_device_mae}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 11.0 & \textbf{8.7} & 25.6 & \textbf{16.4} & 45.6 & 21.5 \\
BiGRU & \textbf{9.8} & 12.8 & 45.6 & 20.6 & 38.9 & 25.5 \\
BiLSTM & 26.5 & 15.6 & 45.7 & 28.6 & 46.5 & 32.6 \\
CNN1D & 25.0 & 32.8 & 32.8 & 32.8 & 40.5 & 32.8 \\
Energformer & 13.5 & 9.0 & 26.5 & 29.4 & 41.4 & 24.0 \\
FCN & 27.4 & 14.6 & 45.6 & 24.9 & 47.5 & 32.0 \\
UNET\_NILM & 25.1 & 13.8 & \textbf{23.3} & 22.7 & 43.0 & 25.6 \\
STNILM & \textbf{9.8} & 8.9 & 36.9 & 21.4 & \textbf{33.3} & 22.1 \\
TSILNet & 18.9 & 15.9 & 24.6 & 28.5 & 38.7 & 25.3 \\
DiffNILM & 15.3 & 16.7 & 28.8 & 34.8 & 59.2 & 31.0 \\
DAResNet & 94.4 & 23.5 & 92.9 & 35.9 & 48.0 & 58.9 \\
\bottomrule
\end{tabular}
}
\end{table}

% F1 table
\begin{table}[htbp]
\centering
\caption{Per-device F1 score on UK-DALE (higher is better). Best result per device in bold is computed over rerun-only rows; fallback rows are excluded from bold ranking.}
\label{tab:per_device_f1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.034 & 0.050 & 0.716 & 0.047 & 0.172 & 0.204 \\
BiGRU & 0.214 & 0.033 & 0.041 & 0.034 & \textbf{0.297} & 0.124 \\
BiLSTM & 0.125 & 0.031 & 0.024 & 0.028 & 0.089 & 0.059 \\
CNN1D & 0.208 & 0.296 & 0.296 & 0.296 & 0.385 & 0.296 \\
Energformer & 0.025 & 0.053 & \textbf{0.762} & \textbf{0.096} & 0.050 & 0.197 \\
FCN & 0.066 & 0.033 & 0.059 & 0.032 & 0.047 & 0.047 \\
UNET\_NILM & 0.207 & 0.031 & 0.706 & 0.034 & 0.272 & 0.250 \\
STNILM & \textbf{0.290} & \textbf{0.065} & 0.638 & 0.094 & 0.041 & 0.226 \\
TSILNet & 0.046 & 0.033 & 0.719 & 0.019 & 0.019 & 0.167 \\
DiffNILM & 0.045 & 0.018 & 0.685 & 0.020 & 0.008 & 0.155 \\
DAResNet & 0.044 & 0.027 & 0.576 & 0.027 & 0.013 & 0.137 \\
\bottomrule
\end{tabular}
}
\end{table}

% Recall table
\begin{table}[htbp]
\centering
\caption{Per-device Recall on UK-DALE (higher is better). Best result per device in bold is computed over rerun-only rows; fallback rows are excluded from bold ranking.}
\label{tab:per_device_recall}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.997 & 0.976 & 0.918 & 0.954 & 0.895 & 0.948 \\
BiGRU & 0.998 & 0.954 & 0.021 & \textbf{0.974} & \textbf{0.981} & 0.786 \\
BiLSTM & 0.066 & 0.935 & 0.012 & 0.843 & 0.146 & 0.400 \\
CNN1D & 0.116 & 0.311 & 0.311 & 0.311 & 0.507 & 0.311 \\
Energformer & \textbf{1.000} & \textbf{0.985} & 0.958 & 0.969 & 0.795 & 0.941 \\
FCN & 0.034 & 0.949 & 0.032 & 0.854 & 0.129 & 0.400 \\
UNET\_NILM & 0.115 & 0.947 & 0.988 & 0.829 & 0.365 & 0.649 \\
STNILM & 0.993 & 0.982 & \textbf{1.000} & 0.907 & 0.887 & 0.954 \\
TSILNet & 0.982 & 0.914 & 0.992 & 0.778 & 0.823 & 0.898 \\
DiffNILM & 0.936 & 0.753 & 0.947 & 0.893 & 0.804 & 0.867 \\
DAResNet & 0.927 & 0.862 & 0.693 & 0.767 & 0.826 & 0.815 \\
\bottomrule
\end{tabular}
}
\end{table}

% SAE table
\begin{table}[htbp]
\centering
\caption{Per-device SAE on UK-DALE (lower is better). Best result per device in bold is computed over rerun-only rows; fallback rows are excluded from bold ranking.}
\label{tab:per_device_sae}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.225 & 0.080 & 0.438 & \textbf{0.695} & 0.840 & 0.456 \\
BiGRU & 0.247 & 0.323 & 0.979 & 1.206 & \textbf{0.078} & 0.567 \\
BiLSTM & 0.936 & 0.607 & 0.987 & 2.107 & 0.794 & 1.086 \\
CNN1D & 0.879 & 0.752 & 0.752 & 0.752 & 0.626 & 0.752 \\
Energformer & 0.123 & 0.112 & 0.390 & 2.764 & 0.784 & 0.835 \\
FCN & 0.969 & 0.818 & 0.964 & 1.906 & 0.811 & 1.094 \\
UNET\_NILM & 0.884 & 0.381 & 0.320 & 1.529 & 0.685 & 0.760 \\
STNILM & 0.276 & \textbf{0.024} & 0.276 & 1.355 & 0.639 & 0.514 \\
TSILNet & \textbf{0.022} & 0.685 & 0.261 & 2.013 & 0.385 & 0.673 \\
DiffNILM & 0.085 & 0.599 & \textbf{0.144} & 2.543 & 0.517 & 0.778 \\
DAResNet & 2.468 & 1.901 & 1.141 & 3.171 & 0.289 & 1.794 \\
\bottomrule
\end{tabular}
}
\end{table}

% Precision table
\begin{table}[htbp]
\centering
\caption{Per-device Precision on UK-DALE (higher is better). Best result per device in bold is computed over rerun-only rows; fallback rows are excluded from bold ranking.}
\label{tab:per_device_precision}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.017 & 0.026 & 0.593 & 0.024 & 0.095 & 0.151 \\
BiGRU & 0.120 & 0.017 & 0.524 & 0.017 & 0.175 & 0.171 \\
BiLSTM & \textbf{1.000} & 0.016 & 0.546 & 0.014 & 0.064 & 0.328 \\
CNN1D & 0.999 & 0.654 & 0.654 & 0.654 & 0.310 & 0.654 \\
Energformer & 0.012 & 0.027 & \textbf{0.633} & \textbf{0.055} & 0.027 & 0.151 \\
FCN & \textbf{1.000} & 0.017 & 0.501 & 0.016 & 0.029 & 0.313 \\
UNET\_NILM & 0.997 & 0.016 & 0.550 & 0.017 & \textbf{0.217} & 0.359 \\
STNILM & 0.175 & \textbf{0.034} & 0.468 & 0.049 & 0.021 & 0.149 \\
TSILNet & 0.024 & 0.017 & 0.563 & 0.010 & 0.010 & 0.125 \\
DiffNILM & 0.023 & 0.009 & 0.538 & 0.010 & 0.004 & 0.117 \\
DAResNet & 0.023 & 0.014 & 0.500 & 0.014 & 0.006 & 0.111 \\
\bottomrule
\end{tabular}
}
\end{table}

Several observations emerge from the per-device baseline results. First, no single baseline dominates across all devices and metrics. BERT4NILM achieves strong NDE on kettle (0.115) and microwave (0.676) but struggles with washing machine (NDE = 1.428). Energformer excels at fridge NDE (0.345) and achieves near-perfect recall across most devices, but its washing machine NDE (2.889) is the worst among all models. This heterogeneity underscores the difficulty of building a universal NILM model under the single-device paradigm.

Second, the washing machine consistently proves challenging across all baselines. Most models achieve NDE $>$ 1.0 on this device, meaning their energy estimation is no better than a trivial predictor. The multi-stage operational profile---including wash, rinse, and spin phases at different power levels---creates a complex temporal pattern that single-device models struggle to capture.

Third, a striking pattern appears in the Precision--Recall columns. Models with high recall (BERT4NILM: 0.948, Energformer: 0.941, STNILM: 0.954) tend to have very low precision (0.151, 0.151, 0.149 respectively), indicating that these models achieve high sensitivity by over-predicting activations. Conversely, models with reasonable precision (BiLSTM: 0.328, CNN1D: 0.654) achieve poor recall (0.400, 0.311), suggesting they under-predict to avoid false positives. This systematic trade-off across baselines contextualizes CondiNILMformer's own Precision--Recall balance.

\subsubsection{CondiNILMformer versus NILMFormer Per-Device}

Table~\ref{tab:per_device} provides a focused head-to-head comparison between CondiNILMformer and its direct predecessor NILMFormer on each of the five target appliances.

\begin{table}[htbp]
\centering
\caption{Per-device performance comparison between NILMFormer and CondiNILMformer on UK-DALE (single-device training). Best results per device in bold.}
\label{tab:per_device}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l c c c c}
\toprule
\textbf{Device} & \textbf{Method} & \textbf{MAE}$\downarrow$ & \textbf{F1}$\uparrow$ & \textbf{Recall}$\uparrow$ & \textbf{NDE}$\downarrow$ \\
\midrule
\multirow{2}{*}{Kettle} & NILMFormer & 18.2 & 0.28 & 0.65 & 0.92 \\
& CondiNILMformer & \textbf{15.7} & \textbf{0.33} & \textbf{0.80} & \textbf{0.78} \\
\midrule
\multirow{2}{*}{Microwave} & NILMFormer & 12.4 & 0.11 & 0.58 & 1.68 \\
& CondiNILMformer & \textbf{9.6} & \textbf{0.13} & \textbf{0.67} & \textbf{1.51} \\
\midrule
\multirow{2}{*}{Fridge} & NILMFormer & 22.1 & 0.76 & 0.95 & 0.41 \\
& CondiNILMformer & \textbf{20.9} & \textbf{0.78} & \textbf{0.96} & \textbf{0.38} \\
\midrule
\multirow{2}{*}{Washing Machine} & NILMFormer & 15.3 & 0.58 & 0.69 & 0.47 \\
& CondiNILMformer & \textbf{13.5} & \textbf{0.62} & \textbf{0.73} & \textbf{0.42} \\
\midrule
\multirow{2}{*}{Dishwasher} & NILMFormer & 13.8 & 0.73 & 0.88 & 0.18 \\
& CondiNILMformer & \textbf{11.5} & \textbf{0.76} & \textbf{0.90} & \textbf{0.16} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/5. Experimental Results/ukdale-per-device-results.png}
\caption{UK-DALE per-device qualitative disaggregation results across the five target appliances.}
\label{fig:ukdale_per_device}
\end{figure}

CondiNILMformer outperforms NILMFormer on all five devices across all four reported metrics. The magnitude of improvement varies systematically by device category, providing insight into which components contribute most to each type of appliance.

\textbf{Sparse high-power appliances.} Kettle and microwave exhibit the largest improvements, consistent with the design focus on sparse device detection. Kettle Recall increases from 0.65 to 0.80 (+23\%), and NDE improves from 0.92 to 0.78 ($\downarrow$15\%). Microwave Recall improves from 0.58 to 0.67 (+16\%), with MAE decreasing from 12.4~W to 9.6~W ($\downarrow$22.6\%). These gains are consistent with the design intent of the AdaptiveDeviceLoss mechanism, which assigns higher loss weights to the rare ON-state samples of sparse devices. The ablation study (Section~\ref{sec:ablation}) provides supporting evidence, though definitive causal attribution would require controlled single-device ablations beyond the scope of this thesis.

\textbf{Periodic appliance.} Fridge shows stable improvements across all metrics: Recall improves from 0.95 to 0.96, F1 from 0.76 to 0.78, and NDE from 0.41 to 0.38 ($\downarrow$7.3\%). The more modest gains reflect the fridge's inherently easier disaggregation profile---its regular cycling pattern provides consistent training signals---while confirming that CondiNILMformer's sparse-device optimizations do not harm periodic device performance.

\textbf{Long-cycle multi-stage appliances.} Washing machine and dishwasher show meaningful improvements. Washing machine F1 improves from 0.58 to 0.62 (+6.9\%), and dishwasher F1 improves from 0.73 to 0.76 (+4.1\%). NDE improves for both devices (washing machine: 0.47 to 0.42; dishwasher: 0.18 to 0.16). These improvements, while smaller than those for sparse devices, demonstrate that the FiLM conditioning and soft gating mechanisms benefit multi-phase appliances by providing better phase-aware feature modulation.

\subsection{Multi-Device Joint Training}
\label{subsec:multi_device}

CondiNILMformer is the only model in this comparison that supports native multi-device joint training, where a single model simultaneously disaggregates all five appliances. This capability is enabled by four architectural components: FiLM conditional modulation injecting device-specific electrical and frequency features, device-specific adapters maintaining per-device representations, type-grouped output heads producing device-category-appropriate predictions, and AdaptiveDeviceLoss with PCGrad for shared-parameter stabilization.

\subsubsection{Multi-Device Overall Results}

Table~\ref{tab:multi_device_full} reports the comprehensive results of the multi-device CondiNILMformer (V8.1 best configuration, epoch 23), including both overall metrics and per-device breakdowns across all 12 evaluation metrics.

\begin{table}[htbp]
\centering
\caption{CondiNILMformer multi-device joint training results on UK-DALE. Overall metrics (left) and per-device breakdown (right). WM = Washing Machine, DW = Dishwasher.}
\label{tab:multi_device_full}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c | c c c c c}
\toprule
\textbf{Metric} & \textbf{Overall} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{WM} & \textbf{DW} \\
\midrule
MAE$\downarrow$ & 20.4 & 32.4 & 20.8 & 25.7 & 11.5 & 11.6 \\
MSE$\downarrow$ & 13622.8 & 27349.8 & 12373.7 & 1747.6 & 13617.5 & 13025.4 \\
RMSE$\downarrow$ & 116.7 & 165.4 & 111.2 & 41.8 & 116.7 & 114.1 \\
NDE$\downarrow$ & 0.398 & 0.878 & 3.269 & 0.423 & 0.318 & 0.146 \\
SAE$\downarrow$ & 0.552 & 1.810 & 5.387 & 0.344 & 0.029 & 0.184 \\
TECA$\uparrow$ & 0.589 & 0.000 & $-$2.089 & 0.685 & 0.746 & 0.859 \\
MR$\downarrow$ & 0.513 & 0.312 & 0.089 & 0.576 & 0.590 & 0.772 \\
Acc$\uparrow$ & 0.890 & 0.987 & 0.973 & 0.754 & 0.960 & 0.988 \\
BAcc$\uparrow$ & 0.894 & 0.956 & 0.841 & 0.776 & 0.751 & 0.970 \\
Prec$\uparrow$ & 0.496 & 0.368 & 0.085 & 0.649 & 0.519 & 0.757 \\
Rec$\uparrow$ & 0.899 & 0.924 & 0.709 & 0.946 & 0.523 & 0.951 \\
F1$\uparrow$ & 0.639 & 0.527 & 0.152 & 0.770 & 0.521 & 0.843 \\
\bottomrule
\end{tabular}
}
\end{table}

The multi-device model achieves an overall F1 of 0.639 with Recall = 0.899 and Balanced Accuracy = 0.894, demonstrating that a single CondiNILMformer model can handle all five appliances with reasonable performance. Per-device analysis reveals substantial variation. Dishwasher achieves excellent disaggregation quality (NDE = 0.146, F1 = 0.843, Recall = 0.951), benefiting from its distinctive power profile and relatively long activation durations. Fridge shows robust tracking (F1 = 0.770, Recall = 0.946), consistent with its regular cycling pattern. Kettle demonstrates strong recall (0.924) despite a challenging NDE (0.878), indicating that the model successfully detects activations but occasionally overestimates their energy content.

Microwave remains the most challenging device in the multi-device setting (NDE = 3.269, F1 = 0.152), consistent with its extremely low duty cycle and the additional gradient competition from joint training. The negative TECA ($-$2.089) indicates that the model predicts substantially more total microwave energy than actually consumed. Washing machine shows moderate performance (F1 = 0.521, NDE = 0.318) with notably balanced Precision (0.519) and Recall (0.523), suggesting that the model neither over-predicts nor under-predicts washing machine activations but struggles with accurate energy estimation during complex multi-phase cycles.

\subsubsection{Multi-Device versus Single-Device Comparison}

Table~\ref{tab:joint_training} compares F1 scores between single-device models and the multi-device joint model. This comparison isolates the effect of joint training from the architectural contributions. \textbf{Interpretation note:} Overall uses micro aggregation, while per-device values use macro/per-device reporting; absolute values are therefore not directly comparable across these aggregation levels, and only within-protocol changes should be interpreted.

\begin{table}[htbp]
\centering
\caption{Comparison of single-device and multi-device F1 scores for CondiNILMformer on UK-DALE. The Overall column (single-device) is from micro aggregation in Table~\ref{tab:overall_comparison}; single-device per-device columns are from Section~\ref{subsec:per_device}; multi-device values are from Table~\ref{tab:multi_device_full}. WM = Washing Machine, DW = Dishwasher.}
\label{tab:joint_training}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Training Mode} & \textbf{Overall} & \textbf{Kettle} & \textbf{Microwave} & \textbf{Fridge} & \textbf{WM} & \textbf{DW} \\
\midrule
Single-device & \textbf{0.74} & 0.33 & 0.13 & \textbf{0.78} & \textbf{0.62} & 0.76 \\
Multi-device Joint & 0.639 & \textbf{0.527} & \textbf{0.152} & 0.770 & 0.521 & \textbf{0.843} \\
\midrule
Change (Multi vs Single) & $-$13.6\% & +59.7\% & +16.9\% & $-$1.3\% & $-$16.0\% & +10.9\% \\
\bottomrule
\end{tabular}
}
\end{table}

Compared with single-device training, multi-device joint training shows device-dependent F1 changes rather than uniform gains. Kettle, microwave, and dishwasher improve, while fridge and washing machine decrease slightly. Overall F1 is lower in the multi-device setting (0.639 versus 0.74), indicating a trade-off between joint optimization and single-task specialization.

These results suggest that shared representations can improve event detection for selected devices, especially sparse appliances, but may also introduce optimization interference for others despite PCGrad balancing. The practical value of the multi-device setting remains its single-model deployment advantage, which reduces training and maintenance overhead.
