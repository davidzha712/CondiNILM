\section{Cross-Dataset Generalization}
\label{sec:cross_dataset}

This section evaluates whether the performance improvements observed on UK-DALE transfer to a different dataset with distinct household compositions, appliance models, and usage patterns. Cross-dataset evaluation is essential for assessing the practical applicability of NILM methods, as real-world deployments must generalize across diverse residential environments.

\subsection{REFIT Dataset Description}
\label{subsec:refit_description}

The REFIT dataset~\cite{Murray2017REFIT} contains electricity consumption records from 20 UK households, offering substantially greater household diversity than UK-DALE's 5 households. The dataset includes aggregate and appliance-level power measurements at 8-second intervals, which are resampled to 1-minute resolution to match the UK-DALE preprocessing. The primary REFIT evaluation set contains four target appliances: kettle, fridge, washing machine, and dishwasher. Microwave is excluded due to insufficient complete records across selected houses.

To keep RQ3 boundaries explicit, this chapter uses two comparison scopes: (i) a four-device REFIT scope for baseline landscape and multi-device transfer analysis; and (ii) a three-device shared scope (fridge, washing machine, dishwasher) for strict CondiNILMformer-vs-NILMFormer head-to-head comparison.
All REFIT results in this chapter use the fixed split manifest in Table~\ref{tab:refit_split_manifest} (train/validation houses \{2, 5, 6, 9\}, test houses \{7, 15\}; fridge single-device baselines use train/validation houses \{2, 5, 9\}).

\subsection{Single-Device Baseline Results on REFIT}
\label{subsec:refit_baselines}

Tables~\ref{tab:refit_nde}--\ref{tab:refit_sae} present per-device results for 11 baselines across six key metrics: NDE, MAE, F1, Recall, Precision, and SAE. Values come from the latest REFIT result bundle (V9), with original-paper fallback values used only where a run was unavailable; this maintains a consistent four-device comparison table.
After completion, coverage is 4/4 devices for each model. In the latest package, fallback completion occurs on the CNN1D row only. Table~\ref{tab:refit_baseline_source} provides explicit row-level source labels (rerun/fallback), and Table~\ref{tab:refit_rerun_leaderboard} reports a strict full-rerun subset leaderboard for hard comparability.

All tables in this section use per-device (macro-style) reporting. Hence, these values should be compared within-section, not against the micro-aggregated overall scores reported for UK-DALE in Section~\ref{subsec:overall_results}.

\begin{table}[htbp]
\centering
\caption{REFIT per-device baseline source annotation (applies to Tables~\ref{tab:refit_nde}--\ref{tab:refit_sae}, including the derived Precision table \ref{tab:refit_precision}).}
\label{tab:refit_baseline_source}
\begin{tabular}{l c c}
\toprule
\textbf{Model} & \textbf{Coverage} & \textbf{Source} \\
\midrule
BERT4NILM & 4/4 & rerun \\
BiGRU & 4/4 & rerun \\
BiLSTM & 4/4 & rerun \\
CNN1D & 4/4 & fallback \\
Energformer & 4/4 & rerun \\
FCN & 4/4 & rerun \\
UNET\_NILM & 4/4 & rerun \\
STNILM & 4/4 & rerun \\
TSILNet & 4/4 & rerun \\
DiffNILM & 4/4 & rerun \\
DAResNet & 4/4 & rerun \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{REFIT full-rerun subset leaderboard (fallback rows excluded), ranked by Avg F1.}
\label{tab:refit_rerun_leaderboard}
\begin{tabular}{c l c c}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Avg F1}$\uparrow$ & \textbf{Avg NDE}$\downarrow$ \\
\midrule
1 & BERT4NILM & 0.173 & 0.728 \\
2 & BiGRU & 0.135 & 0.693 \\
3 & Energformer & 0.135 & 0.726 \\
4 & STNILM & 0.105 & 0.676 \\
5 & UNET\_NILM & 0.048 & 0.707 \\
6 & TSILNet & 0.042 & 0.657 \\
7 & FCN & 0.037 & 0.602 \\
8 & BiLSTM & 0.037 & 0.739 \\
9 & DAResNet & 0.031 & 4.325 \\
10 & DiffNILM & 0.021 & 1.025 \\
\bottomrule
\end{tabular}
\end{table}

% REFIT NDE table
\begin{table}[htbp]
\centering
\caption{Per-device NDE on REFIT (lower is better). Best result per device in bold.}
\label{tab:refit_nde}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.330 & 0.729 & 0.856 & 0.997 & 0.728 \\
BiGRU & \textbf{0.287} & 0.964 & \textbf{0.825} & 0.695 & 0.693 \\
BiLSTM & 0.339 & 0.740 & 0.988 & 0.889 & 0.739 \\
CNN1D & 0.885 & 0.887 & 0.920 & 0.860 & 0.888 \\
Energformer & 0.413 & 0.725 & 0.909 & 0.857 & 0.726 \\
FCN & 0.335 & 0.601 & 0.870 & \textbf{0.602} & \textbf{0.602} \\
UNET\_NILM & 0.378 & 0.708 & 1.040 & 0.702 & 0.707 \\
STNILM & 0.314 & 0.675 & 0.953 & 0.762 & 0.676 \\
TSILNet & 0.339 & 0.658 & 0.896 & 0.735 & 0.657 \\
DiffNILM & 1.011 & 1.024 & 1.043 & 1.022 & 1.025 \\
DAResNet & 0.741 & 4.325 & 2.943 & 9.291 & 4.325 \\
\bottomrule
\end{tabular}
}
\end{table}

% REFIT MAE table
\begin{table}[htbp]
\centering
\caption{Per-device MAE (W) on REFIT (lower is better). Best result per device in bold.}
\label{tab:refit_mae}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 16.9 & 26.7 & 29.9 & 32.9 & 26.6 \\
BiGRU & 9.4 & 19.2 & 53.1 & 44.5 & 31.6 \\
BiLSTM & 18.0 & 43.1 & 40.9 & 70.0 & 43.0 \\
CNN1D & \textbf{8.3} & 27.6 & 39.5 & 35.0 & 27.6 \\
Energformer & 12.4 & 28.5 & 36.9 & 35.8 & 28.4 \\
FCN & 15.5 & 30.1 & 35.5 & 39.3 & 30.1 \\
UNET\_NILM & 15.7 & 36.0 & 42.7 & 49.6 & 36.0 \\
STNILM & 8.8 & 27.5 & 33.0 & 41.1 & 27.6 \\
TSILNet & 17.6 & 33.6 & 35.8 & 47.0 & 33.5 \\
DiffNILM & 33.8 & 50.4 & 48.4 & 69.0 & 50.4 \\
DAResNet & 53.4 & 132.2 & 101.4 & 242.2 & 132.3 \\
\bottomrule
\end{tabular}
}
\end{table}

% REFIT F1 table
\begin{table}[htbp]
\centering
\caption{Per-device F1 score on REFIT (higher is better). Best result per device in bold.}
\label{tab:refit_f1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.071 & 0.173 & \textbf{0.375} & 0.073 & 0.173 \\
BiGRU & 0.091 & 0.092 & 0.323 & 0.032 & 0.135 \\
BiLSTM & 0.030 & 0.037 & 0.030 & 0.051 & 0.037 \\
CNN1D & 0.138 & 0.236 & 0.276 & \textbf{0.298} & \textbf{0.237} \\
Energformer & \textbf{0.216} & 0.134 & 0.039 & 0.151 & 0.135 \\
FCN & 0.029 & 0.038 & 0.035 & 0.046 & 0.037 \\
UNET\_NILM & 0.031 & 0.047 & 0.043 & 0.071 & 0.048 \\
STNILM & 0.147 & 0.104 & 0.036 & 0.133 & 0.105 \\
TSILNet & 0.024 & 0.041 & 0.042 & 0.061 & 0.042 \\
DiffNILM & 0.020 & 0.021 & 0.020 & 0.023 & 0.021 \\
DAResNet & 0.031 & 0.030 & 0.030 & 0.033 & 0.031 \\
\bottomrule
\end{tabular}
}
\end{table}

% REFIT Recall table
\begin{table}[htbp]
\centering
\caption{Per-device Recall on REFIT (higher is better). Best result per device in bold.}
\label{tab:refit_recall}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.977 & 0.511 & 0.478 & 0.082 & 0.512 \\
BiGRU & 0.971 & 0.049 & 0.979 & 0.954 & 0.738 \\
BiLSTM & 0.945 & 0.920 & 0.893 & 0.926 & 0.921 \\
CNN1D & 0.075 & 0.308 & 0.516 & 0.337 & 0.309 \\
Energformer & 0.934 & 0.946 & 0.925 & 0.979 & 0.946 \\
FCN & 0.960 & 0.917 & 0.847 & 0.940 & 0.916 \\
UNET\_NILM & 0.946 & 0.914 & 0.858 & 0.938 & 0.914 \\
STNILM & \textbf{0.988} & \textbf{0.981} & \textbf{0.971} & \textbf{0.984} & \textbf{0.981} \\
TSILNet & 0.972 & 0.955 & 0.908 & 0.985 & 0.955 \\
DiffNILM & 0.664 & 0.694 & 0.722 & 0.696 & 0.694 \\
DAResNet & 0.919 & 0.769 & 0.695 & 0.693 & 0.769 \\
\bottomrule
\end{tabular}
}
\end{table}

% REFIT Precision table (derived)
\paragraph{Derived Precision (from F1 and Recall).}
To complete the baseline metric set, Precision is derived from F1 and Recall using:
\begin{equation*}
P = \frac{F_1 \cdot R}{2R - F_1}
\end{equation*}
Because F1 and Recall in Tables~\ref{tab:refit_f1}--\ref{tab:refit_recall} are rounded, the derived Precision values are approximate.

\begin{table}[htbp]
\centering
\caption{Per-device Precision on REFIT (higher is better), derived from F1 and Recall. Values are approximate due to rounding in the source F1/Recall tables. Best result per device in bold.}
\label{tab:refit_precision}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.037 & 0.104 & \textbf{0.309} & 0.066 & 0.129 \\
BiGRU & 0.048 & \textbf{0.751} & 0.193 & 0.016 & 0.252 \\
BiLSTM & 0.015 & 0.019 & 0.015 & 0.026 & 0.019 \\
CNN1D & \textbf{0.863} & 0.191 & 0.188 & \textbf{0.267} & \textbf{0.377} \\
Energformer & 0.122 & 0.072 & 0.020 & 0.082 & 0.074 \\
FCN & 0.015 & 0.019 & 0.018 & 0.024 & 0.019 \\
UNET\_NILM & 0.016 & 0.024 & 0.022 & 0.037 & 0.025 \\
STNILM & 0.079 & 0.055 & 0.018 & 0.071 & 0.056 \\
TSILNet & 0.012 & 0.021 & 0.021 & 0.031 & 0.022 \\
DiffNILM & 0.010 & 0.011 & 0.010 & 0.012 & 0.011 \\
DAResNet & 0.016 & 0.015 & 0.015 & 0.017 & 0.016 \\
\bottomrule
\end{tabular}
}
\end{table}

% REFIT SAE table
\begin{table}[htbp]
\centering
\caption{Per-device SAE on REFIT (lower is better). Best result per device in bold.}
\label{tab:refit_sae}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{Kettle} & \textbf{Fridge} & \textbf{Washing Machine} & \textbf{Dishwasher} & \textbf{Avg} \\
\midrule
BERT4NILM & 0.592 & 0.719 & 0.651 & 0.918 & 0.720 \\
BiGRU & \textbf{0.096} & 0.935 & 0.616 & 1.195 & 0.710 \\
BiLSTM & 0.595 & 1.048 & 0.697 & 1.856 & 1.049 \\
CNN1D & 0.895 & 0.571 & \textbf{0.213} & 0.601 & 0.570 \\
Energformer & 0.110 & 0.329 & 0.443 & 0.438 & 0.330 \\
FCN & 0.389 & 0.422 & 0.462 & 0.415 & 0.422 \\
UNET\_NILM & 0.321 & 0.519 & 0.781 & 0.455 & 0.519 \\
STNILM & \textbf{0.095} & \textbf{0.176} & 0.301 & \textbf{0.136} & \textbf{0.177} \\
TSILNet & 0.583 & 0.748 & 0.401 & 1.264 & 0.749 \\
DiffNILM & 0.821 & 0.953 & 1.242 & 0.792 & 0.952 \\
DAResNet & 2.811 & 4.373 & 3.468 & 6.840 & 4.373 \\
\bottomrule
\end{tabular}
}
\end{table}

The REFIT baseline landscape exhibits several patterns that parallel UK-DALE while revealing dataset-specific challenges. First, most baselines achieve notably higher NDE on REFIT than on UK-DALE, reflecting the greater household diversity and more variable appliance usage patterns in REFIT's 20-household collection. Second, the same Precision--Recall imbalance observed on UK-DALE persists on REFIT: models such as STNILM achieve very high recall (0.981 average; Table~\ref{tab:refit_recall}) but low derived Precision (0.056 average; Table~\ref{tab:refit_precision}) and low F1 (0.105 average; Table~\ref{tab:refit_f1}), indicating pervasive over-prediction. Third, because the CNN1D row is fallback-filled, Tables~\ref{tab:refit_nde}--\ref{tab:refit_sae} are treated as a \textit{broad landscape}, Table~\ref{tab:refit_rerun_leaderboard} is used for strict rerun-only baseline ranking, and Table~\ref{tab:refit_results} serves as the strict shared-device head-to-head comparison.

\subsection{CondiNILMformer versus NILMFormer on REFIT}
\label{subsec:refit_condi_vs_nilm}

Table~\ref{tab:refit_results} provides the direct comparison between CondiNILMformer and NILMFormer on REFIT. Three devices with complete data for both methods are evaluated: fridge, washing machine, and dishwasher.

\begin{table}[htbp]
\centering
\caption{Per-device performance comparison between NILMFormer and CondiNILMformer on REFIT (single-device training). Best results per device in bold.}
\label{tab:refit_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l c c c}
\toprule
\textbf{Device} & \textbf{Method} & \textbf{MAE}$\downarrow$ & \textbf{F1}$\uparrow$ & \textbf{Recall}$\uparrow$ \\
\midrule
\multirow{2}{*}{Fridge} & NILMFormer & 24.8 & 0.71 & 0.89 \\
& CondiNILMformer & \textbf{22.3} & \textbf{0.74} & \textbf{0.92} \\
\midrule
\multirow{2}{*}{Washing Machine} & NILMFormer & 18.6 & 0.53 & 0.64 \\
& CondiNILMformer & \textbf{16.2} & \textbf{0.58} & \textbf{0.70} \\
\midrule
\multirow{2}{*}{Dishwasher} & NILMFormer & 16.1 & 0.68 & 0.82 \\
& CondiNILMformer & \textbf{14.5} & \textbf{0.72} & \textbf{0.86} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/5. Experimental Results/refit-per-device-results.png}
\caption{REFIT per-device qualitative disaggregation results on the selected target appliances.}
\label{fig:refit_per_device}
\end{figure}

CondiNILMformer consistently outperforms NILMFormer on all three REFIT devices across all reported metrics. The improvement magnitudes are comparable to those observed on UK-DALE:

\begin{itemize}
    \item \textbf{Fridge}: MAE decreases from 24.8~W to 22.3~W ($\downarrow$10.1\%), F1 improves from 0.71 to 0.74 ($\uparrow$4.2\%), and Recall increases from 0.89 to 0.92 ($\uparrow$3.4\%).
    \item \textbf{Washing Machine}: MAE decreases from 18.6~W to 16.2~W ($\downarrow$12.9\%), F1 improves from 0.53 to 0.58 ($\uparrow$9.4\%), and Recall increases from 0.64 to 0.70 ($\uparrow$9.4\%).
    \item \textbf{Dishwasher}: MAE decreases from 16.1~W to 14.5~W ($\downarrow$9.9\%), F1 improves from 0.68 to 0.72 ($\uparrow$5.9\%), and Recall increases from 0.82 to 0.86 ($\uparrow$4.9\%).
\end{itemize}

Averaged across the three shared devices, MAE decreases by 10.9\% (19.8~W to 17.7~W) and F1 improves by 6.3\%. The consistency of these improvements across a dataset with different households, appliance models, and usage distributions than UK-DALE suggests---under the current evaluation protocol and seed-42 setting---that CondiNILMformer's gains are not purely dataset-specific artifacts. However, as both datasets are from UK households, further evaluation on geographically and demographically diverse datasets would be needed to confirm broad generalisability.

\subsection{Multi-Device Results on REFIT}
\label{subsec:refit_multi}

Table~\ref{tab:refit_multi} reports the multi-device joint training results on REFIT, evaluating whether the multi-device capability demonstrated on UK-DALE transfers to a different dataset. The REFIT multi-device model jointly handles four appliances (kettle, fridge, washing machine, dishwasher) using the V8.1 best configuration (epoch 14).

\begin{table}[htbp]
\centering
\caption{CondiNILMformer multi-device joint training results on REFIT. Overall metrics and per-device breakdown. WM = Washing Machine, DW = Dishwasher.}
\label{tab:refit_multi}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c | c c c c}
\toprule
\textbf{Metric} & \textbf{Overall} & \textbf{Kettle} & \textbf{Fridge} & \textbf{WM} & \textbf{DW} \\
\midrule
MAE$\downarrow$ & 21.9 & 17.4 & 31.0 & 19.9 & 19.2 \\
MSE$\downarrow$ & 18436.2 & 20512.7 & 2488.2 & 25498.7 & 25245.2 \\
RMSE$\downarrow$ & 135.8 & 143.2 & 49.9 & 159.7 & 158.9 \\
NDE$\downarrow$ & 0.480 & 0.388 & 0.628 & 0.954 & 0.360 \\
SAE$\downarrow$ & 0.087 & 0.207 & 0.283 & 0.249 & 0.029 \\
TECA$\uparrow$ & 0.617 & 0.632 & 0.588 & 0.452 & 0.724 \\
MR$\downarrow$ & 0.463 & 0.500 & 0.469 & 0.230 & 0.562 \\
Acc$\uparrow$ & 0.900 & 0.993 & 0.709 & 0.950 & 0.969 \\
BAcc$\uparrow$ & 0.836 & 0.868 & 0.722 & 0.559 & 0.813 \\
Prec$\uparrow$ & 0.595 & 0.658 & 0.625 & 0.443 & 0.629 \\
Rec$\uparrow$ & 0.749 & 0.740 & 0.819 & 0.126 & 0.643 \\
F1$\uparrow$ & 0.663 & 0.696 & 0.709 & 0.196 & 0.636 \\
\bottomrule
\end{tabular}
}
\end{table}

The REFIT multi-device model achieves an overall F1 of 0.663, Recall of 0.749, and Balanced Accuracy of 0.836. Compared with the UK-DALE multi-device results (F1 = 0.639, Recall = 0.899), the REFIT model shows higher F1 but lower Recall, reflecting the different device composition (four devices instead of five, without the challenging microwave).

Per-device analysis reveals device-specific patterns. Kettle achieves strong F1 (0.696) and high Balanced Accuracy (0.868), demonstrating effective sparse device detection on a previously unseen dataset. Fridge exhibits robust tracking (F1 = 0.709, Recall = 0.819), consistent with its regular cycling behavior. Dishwasher maintains good disaggregation quality (F1 = 0.636, NDE = 0.360) with low SAE (0.029), indicating accurate total energy estimation.

Washing machine presents the main challenge (F1 = 0.196, Recall = 0.126), substantially worse than on UK-DALE (F1 = 0.521, Recall = 0.523). The low Recall suggests that the model frequently misses washing machine activations on REFIT, potentially due to greater variability in washing machine usage patterns across REFIT's 20 households compared with UK-DALE's more homogeneous training set. The NDE of 0.954 is near the trivial-predictor threshold, indicating that washing machine disaggregation in the REFIT multi-device setting requires further investigation.

Despite the washing machine challenge, the overall REFIT multi-device results confirm that CondiNILMformer's joint training capability generalizes beyond UK-DALE. The model produces a single deployable solution for four-device disaggregation with an overall F1 of 0.663 and SAE of 0.087, the latter indicating that total energy estimation accuracy is substantially better on REFIT than on UK-DALE (SAE = 0.552). This improvement likely reflects the absence of microwave---the most challenging device for energy estimation---from the REFIT evaluation set.
