\section{Discussion}
\label{sec:discussion}

This section synthesises findings from the preceding experiments, analyses trade-offs inherent in the proposed approach, examines failure cases, and provides practical configuration recommendations. Unless stated otherwise, interpretations are made under the deterministic seed-42 protocol and should be read as trend-level evidence.

\subsection{Analysis of Method Strengths}
\label{subsec:strengths}

The experimental results across two datasets and multiple evaluation settings reveal four consistent strengths of CondiNILMformer.

\textbf{Domain knowledge integration.} FiLM conditioning enables the model to inject electrical and frequency-domain features into the Transformer backbone, providing device-aware processing that purely data-driven approaches lack. The ablation study (Section~\ref{sec:ablation}) confirms this design is critical: removing FiLM degrades NDE by 126\%, and frequency-domain conditioning alone achieves the best NDE (0.372) across all configurations. This demonstrates that domain-informed feature modulation can substantially improve disaggregation without requiring architectural complexity beyond affine transformations.

\textbf{Multi-device capability.} CondiNILMformer is the only evaluated model supporting native multi-device joint training. The joint training results (Section~\ref{subsec:multi_device}) show that a single model can handle all five appliances simultaneously with F1 = 0.639 and Recall = 0.899. Compared with single-device training (overall F1 = 0.74), this reflects a performance--efficiency trade-off rather than universal F1 gains. The practical implication remains significant: deploying one model instead of five reduces inference cost, memory footprint, and maintenance complexity in real-world NILM systems.

\textbf{Sparse device detection.} The most significant quantitative improvement is in Recall for sparse high-power appliances. On UK-DALE, CondiNILMformer achieves an overall Recall of 0.93, a 17.7\% improvement over NILMFormer's 0.79. Per-device analysis reveals that sparse appliances benefit most: Kettle Recall improves by 23\% (0.65 to 0.80) and Microwave Recall improves by 16\% (0.58 to 0.67). The AdaptiveDeviceLoss and soft gating mechanism work in concert to prevent the near-zero prediction tendency that plagues conventional models on low-duty-cycle devices.

\textbf{Cross-dataset generalisation.} Performance improvements are not limited to UK-DALE. On REFIT shared devices (fridge, washing machine, dishwasher), CondiNILMformer reduces MAE by 10.9\% and improves F1 by 6.3\% compared with NILMFormer, with consistent gains across all evaluated devices (Section~\ref{sec:cross_dataset}). The multi-device model also transfers effectively, achieving F1 = 0.663 on REFIT without dataset-specific tuning. These results suggest that the proposed method captures partially generalisable device signatures rather than purely dataset-specific artefacts.

\subsection{Precision--Recall Trade-off Analysis}
\label{subsec:precision_recall}

The most prominent trade-off in CondiNILMformer is between Precision and Recall. On UK-DALE, the model achieves Recall = 0.93 but Precision = 0.61, compared with NILMFormer's more balanced Recall = 0.79 and Precision = 0.67. This trade-off arises from two design choices.

First, AdaptiveDeviceLoss emphasises recall for sparse devices by assigning higher loss weights to ON-state samples that are underrepresented in the training data. This reduces missed detections but increases false positives, as the model becomes more sensitive to any input pattern resembling an activation.

Second, the soft gating mechanism, while effective at separating energy estimation from state detection (NDE improves by 43\% with gating), operates with a bias toward predicting activations. The ablation variant without soft gate (A4) achieves higher Precision (0.661 versus 0.496) at the cost of substantially worse NDE.

The practical acceptability of this trade-off depends on the application context. For energy accounting and billing disaggregation, high Recall is preferable because missed activations directly cause estimation errors. For demand response or anomaly detection, higher Precision may be required. The loss weight parameters in AdaptiveDeviceLoss can be adjusted to shift along the Precision--Recall frontier, providing deployment flexibility.

\subsection{Multi-Device versus Single-Device Trade-off}
\label{subsec:multi_single_tradeoff}

Multi-device joint training introduces a trade-off between model efficiency and per-device optimisation. The multi-device model achieves an overall NDE of 0.398, which is higher than the best single-device average NDE of 0.37. This gap reflects the additional optimisation challenge of jointly satisfying five device-specific objectives.

Per-device analysis reveals that the NDE gap is not uniform. Devices with distinctive power signatures (Dishwasher: NDE = 0.146 in multi-device versus 0.16 in single-device) improve under joint training, suggesting that cross-device contrast can help the model learn clearer decision boundaries. Conversely, devices with overlapping power ranges or ambiguous activation patterns (Microwave: NDE = 3.269 in multi-device) suffer more from gradient competition despite PCGrad mitigation.

The F1 comparison is mixed (Table~\ref{tab:joint_training}): multi-device training improves kettle, microwave, and dishwasher, but decreases fridge and washing machine; overall F1 is lower (0.639 versus 0.74). This suggests that cross-device knowledge transfer can benefit event detection for selected devices, but not uniformly across all appliance types. The practical implication is that multi-device deployment is recommended when operational simplicity is the primary objective, while single-device models remain preferable when peak per-device accuracy is the main priority.

\subsection{Failure Case Analysis}
\label{subsec:failures}

Despite the overall improvements, three recurring failure patterns are identified across both datasets.

\textbf{Overlapping high-power activations.} When two sparse appliances activate simultaneously (e.g., kettle and microwave), the aggregate power signal contains a superposition that is inherently ambiguous at 1-minute resolution. FiLM conditioning can partially distinguish these events through frequency signatures, but temporal overlap still causes attribution errors. This limitation is fundamental to single-channel NILM at low sampling rates.

\textbf{Atypical usage patterns.} Appliance usage that deviates substantially from training distributions degrades reliability. Examples include partial kettle boils (which produce lower and shorter power peaks than full boils), interrupted washing machine cycles, and fridge defrost cycles that create unusual power profiles. The model's conditional modulation is trained on typical patterns and does not generalise reliably to rare operational modes.

\textbf{Microwave challenges.} Microwave remains the most difficult appliance across all experimental settings (single-device F1 = 0.13, multi-device F1 = 0.152). The extremely low duty cycle ($<$3\% of the recording) means that even small numbers of false positives substantially reduce Precision, while the short activation duration (typically 1--3 minutes) provides minimal temporal context for the model. The multi-device NDE for microwave (3.269) indicates that the model predicts more total microwave energy than actually consumed, consistent with false positive detections on other short-duration power events.

\subsection{Objective-Driven Configuration Recommendations}
\label{subsec:recommended_config}

Because different ablation variants optimise different metrics, deployment configuration should be selected by objective rather than by a single universal setting. Table~\ref{tab:recommended_config} summarises objective-driven recommendations under the current seed-42 evidence scope.

\begin{table}[htbp]
\centering
\caption{Objective-driven CondiNILMformer configuration recommendations under seed-42 evidence scope.}
\label{tab:recommended_config}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l l}
\toprule
\textbf{Deployment Objective} & \textbf{Recommendation} & \textbf{Rationale} \\
\midrule
NDE/Recall priority & A7 (Freq FiLM only) & Best NDE (0.372) and highest Recall (0.955) in seed-42 ablation (Section~\ref{sec:ablation}) \\
Precision priority & A4 (w/o Soft Gate) or reduce ON-state weight in AdaptiveDeviceLoss & A4 improves Precision (0.661 vs 0.496 full model) in seed-42 results; ON-weight reduction gives a similar direction for operating-point shift \\
MAE/SAE priority & A3 (w/o Seq2SubSeq) & Best MAE (18.3) and SAE (0.094) in seed-42 ablation (Section~\ref{sec:ablation}) \\
Stability priority & Full model (AdaptiveDeviceLoss + PCGrad; seed-42 stability-critical) & In the current seed-42 UK-DALE five-device setting, removing either component causes collapse; broader generality should be checked with multi-seed validation \\
\bottomrule
\end{tabular}
}
\end{table}

Deployment objective should be defined first, then the corresponding variant should be selected: A7 for NDE/Recall, A4 or lower ON-state weighting for Precision, A3 for MAE/SAE, and the full model for stability.
