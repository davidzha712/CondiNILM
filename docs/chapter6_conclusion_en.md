# Chapter 6: Summary

Non-Intrusive Load Monitoring, as a key technology for smart grids and building energy management, infers the individual power consumption of appliances by analyzing aggregate power signals from building main meters, enabling fine-grained electricity monitoring without installing dedicated meters on each device. This technology holds significant value for improving energy efficiency, promoting user energy-saving behavior changes, and supporting grid demand response. However, three core challenges in multi-device joint disaggregation scenarios—gradient conflict, absence of domain knowledge, and prediction collapse—severely constrain the practical performance of existing methods. Addressing these challenges, this thesis proposes the CondiNILMformer method, achieving significant improvements over existing methods through three synergistic technical innovations: conditional feature modulation mechanism, multi-level adaptive architecture, and device-aware loss function.

The first core contribution of this thesis is the FiLM-based conditional feature modulation mechanism. Existing end-to-end deep learning methods attempt to automatically learn feature representations entirely from raw power data, neglecting the load characteristic analysis methods accumulated over decades in the electrical engineering field. This thesis introduces the Feature-wise Linear Modulation mechanism into NILM tasks for the first time, designing a 13-dimensional conditioning vector comprising 5-dimensional electrical features and 8-dimensional frequency-domain features. Electrical features include mean power, power standard deviation, RMS power, peak power, and crest factor, characterizing temporal statistical properties of loads from different perspectives; frequency-domain features decompose power signals into 8 frequency band magnitudes through Fourier transform, capturing periodic operating patterns and harmonic characteristics of devices. Through concatenation and MLP mapping of learnable device embeddings with conditioning features, the model can generate independent scaling and shifting parameters for each target device, dynamically adjusting network feature processing based on current input electrical characteristics. This design enables the model to leverage load discrimination knowledge validated through practice, improving sample efficiency and interpretability. Ablation experiments demonstrate this mechanism contributes approximately 2.7% F1 score improvement.

The second core contribution is the multi-level device adaptive architecture. The original NILMformer employs a fully shared parameter design lacking mechanisms for handling device differences, causing different devices to produce contradictory gradient updates for shared parameters. This thesis designs a hierarchical parameter sharing strategy combining "shared and specialized components" to address this problem. At the lower level, multi-scale dilated convolution embeddings and Transformer encoders are shared across all devices to learn universal power sequence representations; four convolution layers with dilation rates of 1, 2, 4, and 8 provide an effective receptive field of approximately 60 time steps, combined with diagonally-masked self-attention mechanisms that prohibit self-attention to force the model to learn contextual relationships. At the middle level, lightweight bottleneck-structure adapters are introduced for each target device, fusing with shared features through small-weight residual connections to introduce device-specific adjustments while maintaining shared representations, with adapter parameters accounting for only about 5% of encoder parameters. At the upper level, devices are categorized into sparse high-power, periodic, long-cycle, and other types based on electrical characteristics; same-type devices share output head parameters to leverage similarity, while different types use independent parameters to accommodate differences. The soft gating fusion mechanism is an important component of this architecture, decomposing power prediction into power estimation and state detection subtasks, fusing outputs through parameterized sigmoid transformation with floor, scale factor, and bias parameters, with different device types configured with different gating parameters. Ablation experiments demonstrate the soft gating mechanism contributes approximately 5.4% F1 score improvement, serving as a key factor for sparse device detection capability improvement.

The third core contribution is the device-aware composite loss function. Standard regression loss functions treat all time points and all devices equally, failing to account for the class imbalance problem prevalent in NILM tasks, causing models to tend toward near-zero predictions to minimize overall loss. The six-component composite loss function designed in this thesis constrains model learning from multiple perspectives: main regression loss distinguishes ON and OFF periods through soft threshold functions with separate weighting to address class imbalance; global stability loss provides unweighted baseline supervision signals ensuring early training stability; ON recall loss specifically penalizes underestimation during ON periods to prevent prediction collapse; OFF false positive loss penalizes overestimation during OFF periods to control false alarm rate; ON power accuracy loss uses relative error to directly optimize normalized disaggregation error metrics; energy regression loss constrains total energy prediction within windows ensuring energy conservation. More importantly, this thesis proposes an automatic parameter derivation mechanism based on device electrical statistics, computing duty cycle, peak power, mean ON power, power coefficient of variation, and event duration for each device from training data, automatically classifying devices based on these statistics and configuring corresponding loss parameters to achieve fine-grained "different devices, different optimization strategies" training. Combined with the three-level anti-collapse mechanism comprising loss function level, auxiliary penalty level, and dynamic decay level, prediction collapse problems in sparse device training are effectively addressed. Ablation experiments demonstrate the composite loss function is the most critical factor for performance improvement, with F1 score dropping 6.8% upon removal, with sparse device performance declining particularly significantly.

Systematic experiments on three public NILM datasets—UK-DALE, REFIT, and REDD—comprehensively validate the effectiveness of the proposed method. The UK-DALE dataset contains 2-4 years of electricity records from 5 UK households and is one of the most influential benchmark datasets in the NILM field; the REFIT dataset contains two years of electricity records from 20 UK households, providing richer diversity in household electricity usage patterns; the REDD dataset comes from 6 US households, used to verify cross-regional generalization capability. Experiments adopt house-level data partitioning to ensure training and test sets come from different houses, with target devices including five representative household appliances—kettle, microwave, fridge, washing machine, and dishwasher—covering typical electricity usage patterns including sparse high-power, periodic, and long-cycle multi-stage.

In the UK-DALE main experiments, CondiNILMformer achieves optimal comprehensive performance among 13 baseline methods, with MAE decreasing by 11.4% (from 15.8 to 14.0), F1 score improving by 2.8% (from 0.72 to 0.74), and recall dramatically improving from 0.79 to 0.93 compared to the original NILMformer. Per-device analysis reveals the core strengths of the method: for sparse high-power devices, kettle recall improves from 0.65 to 0.80 and microwave recall improves from 0.58 to 0.67, effectively addressing the missed detection problem of existing methods on sparse devices; for periodic devices, fridge further reduces MAE and NDE while maintaining 0.96 high recall, indicating the model can more accurately track periodic power fluctuations of compressor cycling; for long-cycle devices, washing machine and dishwasher F1 scores improve by 6.9% and 4.1% respectively. Comparison experiments between multi-device joint training and single-device independent training demonstrate that joint training achieves performance equal to or better than single-device training across all devices, validating that device adapter and type-grouped output head designs can effectively mitigate gradient conflict problems in multi-task learning.

Cross-dataset generalization experiments further demonstrate the method's universality. On the REFIT dataset, average MAE decreases by 11.8% and average F1 score improves by 5.7% for three devices—fridge, washing machine, and dishwasher—with performance improvement trends consistent with UK-DALE. On the REDD dataset, despite shorter recording duration and variable data quality, the method still achieves consistent performance improvements, particularly on the most challenging microwave device where recall improves from 0.51 to 0.58. These results demonstrate that the method does not depend on specific dataset statistical properties and possesses good cross-dataset and cross-regional generalization capability.

Ablation experiments systematically analyze the contributions of innovative components and their interaction effects. Sequentially removing components from the complete model reveals that composite loss function contributes most (F1 drops 6.8%), followed by soft gating mechanism (F1 drops 5.4%), with FiLM conditional modulation (F1 drops 2.7%), device adapters (F1 drops 1.4%), and type-grouped output heads (F1 drops 2.7%) also providing stable contributions. More importantly, combined ablation experiments reveal positive synergistic effects between components: simultaneously removing multiple components causes performance degradation greater than the sum of individual contributions—for example, simultaneously removing FiLM and composite loss causes F1 to drop 12.2%, greater than the sum of individual contributions of 9.5%. This result validates the systematic and comprehensive nature of method design, with device-aware features provided by conditional modulation requiring coordination with device-aware loss functions to achieve maximum effect.

In summary, the CondiNILMformer method proposed in this thesis provides an effective solution for multi-device non-intrusive load monitoring tasks through organic integration of domain knowledge with deep learning. Three core technical innovations work synergistically to form a complete methodological system: FiLM conditional modulation mechanism bridges electrical engineering experience with data-driven learning, multi-level adaptive architecture addresses gradient conflict problems in multi-device joint training, and device-aware composite loss function addresses class imbalance and prediction collapse problems. Systematic experimental evaluation validates the method's effectiveness and generalization capability across three public datasets, with particularly significant improvements in sparse device detection. Research outcomes not only advance technological progress in the NILM field but also provide a referenceable methodological framework for other sequence modeling tasks requiring domain knowledge integration.
